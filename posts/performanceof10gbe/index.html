<!doctype html><html lang=zh-cn dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Performance of 10 G Ethernet Using Commodity Hardware | MyNotes</title><meta name=keywords content="ethernet,performance,nic"><meta name=description content="1 INTRODUCTION 2 NETWORK PROCESSING IN THE LINUX KERNEL 2.1 Kernel Accounting 2.2 Packet Transmission 发送 2.3 Packet Reception 接收 2.4 Incorrect SoftIRQ Accounting 3 TESTBED AND KERNEL CONFIGURATION 4 UDP TRANSMISSION 5 TCP TRANSMISSION 6 TCP TRANSMISSION WITH ZERO-COPY 7 THE HARDWARE OFFLOAD FUNCTIONALITIES OF THE NETWORK ADAPTER 8 FIBRE CHANNEL TO 10 GIGABIT ETHERNET TESTS 9 CONCLUSIONS 本文为摘录，原文为"><meta name=author content><link rel=canonical href=https://yangyingchao.github.io/posts/performanceof10gbe/><link crossorigin=anonymous href=/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://yangyingchao.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yangyingchao.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yangyingchao.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://yangyingchao.github.io/apple-touch-icon.png><link rel=mask-icon href=https://yangyingchao.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Performance of 10 G Ethernet Using Commodity Hardware"><meta property="og:description" content="1 INTRODUCTION 2 NETWORK PROCESSING IN THE LINUX KERNEL 2.1 Kernel Accounting 2.2 Packet Transmission 发送 2.3 Packet Reception 接收 2.4 Incorrect SoftIRQ Accounting 3 TESTBED AND KERNEL CONFIGURATION 4 UDP TRANSMISSION 5 TCP TRANSMISSION 6 TCP TRANSMISSION WITH ZERO-COPY 7 THE HARDWARE OFFLOAD FUNCTIONALITIES OF THE NETWORK ADAPTER 8 FIBRE CHANNEL TO 10 GIGABIT ETHERNET TESTS 9 CONCLUSIONS 本文为摘录，原文为"><meta property="og:type" content="article"><meta property="og:url" content="https://yangyingchao.github.io/posts/performanceof10gbe/"><meta property="article:section" content="posts"><meta name=twitter:card content="summary"><meta name=twitter:title content="Performance of 10 G Ethernet Using Commodity Hardware"><meta name=twitter:description content="1 INTRODUCTION 2 NETWORK PROCESSING IN THE LINUX KERNEL 2.1 Kernel Accounting 2.2 Packet Transmission 发送 2.3 Packet Reception 接收 2.4 Incorrect SoftIRQ Accounting 3 TESTBED AND KERNEL CONFIGURATION 4 UDP TRANSMISSION 5 TCP TRANSMISSION 6 TCP TRANSMISSION WITH ZERO-COPY 7 THE HARDWARE OFFLOAD FUNCTIONALITIES OF THE NETWORK ADAPTER 8 FIBRE CHANNEL TO 10 GIGABIT ETHERNET TESTS 9 CONCLUSIONS 本文为摘录，原文为"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://yangyingchao.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Performance of 10 G Ethernet Using Commodity Hardware","item":"https://yangyingchao.github.io/posts/performanceof10gbe/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Performance of 10 G Ethernet Using Commodity Hardware","name":"Performance of 10 G Ethernet Using Commodity Hardware","description":"1 INTRODUCTION 2 NETWORK PROCESSING IN THE LINUX KERNEL 2.1 Kernel Accounting 2.2 Packet Transmission 发送 2.3 Packet Reception 接收 2.4 Incorrect SoftIRQ Accounting 3 TESTBED AND KERNEL CONFIGURATION 4 UDP TRANSMISSION 5 TCP TRANSMISSION 6 TCP TRANSMISSION WITH ZERO-COPY 7 THE HARDWARE OFFLOAD FUNCTIONALITIES OF THE NETWORK ADAPTER 8 FIBRE CHANNEL TO 10 GIGABIT ETHERNET TESTS 9 CONCLUSIONS 本文为摘录，原文为","keywords":["ethernet","performance","nic"],"articleBody":" 1 INTRODUCTION 2 NETWORK PROCESSING IN THE LINUX KERNEL 2.1 Kernel Accounting 2.2 Packet Transmission 发送 2.3 Packet Reception 接收 2.4 Incorrect SoftIRQ Accounting 3 TESTBED AND KERNEL CONFIGURATION 4 UDP TRANSMISSION 5 TCP TRANSMISSION 6 TCP TRANSMISSION WITH ZERO-COPY 7 THE HARDWARE OFFLOAD FUNCTIONALITIES OF THE NETWORK ADAPTER 8 FIBRE CHANNEL TO 10 GIGABIT ETHERNET TESTS 9 CONCLUSIONS 本文为摘录，原文为： ../pdf/3/performanceOf10GbE.pdf\n1 INTRODUCTION With the introduction of 10-GbE, network I/O re-entered the “fast network, slow host” scenario that occurred with both the transitions to Fast Ethernet and Gigabit Ethernet. Specifically, three major system bottlenecks may limit the efficiency of high-performance I/O adapters:\nPCI-X 总线带宽\nPCI-X 频率 133MHz ，带宽 8.5Gb/s 已被 PCI-Express (PCIe) 替代： 8 通道 20 Gb/s CPU 利用率\n内存带宽\n2 NETWORK PROCESSING IN THE LINUX KERNEL 2.1 Kernel Accounting User\n程序运行在用户态的时间 System\n程序运行在内核态的时间 IRQ\nCPU 处理硬件中断的时间 SoftIRQ\nCPU 处理软中断的时间 2.2 Packet Transmission 发送 发送队列\n每个网卡驱动维持一个数据包的发送队列 内核根据 qdisc (queue discipline) 将数据包插入到队列中 默认的 qdisc 为 pfifo_fast (paccket FIO) Linux 支持其他的策略, 如 RED (Radom Early Drop) CBQ (Class Based Queuing) Others link layer\ntriggered by function dev_queue_xmit(), 该函数负责： 将数据包根据 qdisc 插入到发送队列中 从发送队列中取出待发送的包，调用驱动的发送函数 hard_start_xmit() 来发送 如果因为某些原因 （如设备没有资源了），它会安排 SoftIRQ ， 然后晚些通过软中断再次发送 设备驱动\n设备驱动负责将数据从 tx_ring 中转移到网卡的缓冲区中 该操作通过设置 DMA 映射，然后设置硬件上的特定寄存器来完成 驱动无需等待发送完成： 发送完成之后硬件发出硬中断 DMAdone 内核收到中断后安排 SoftIRQ 来释放 packet 使用的内存 释放内存操作耗时相对较长，不适合在硬中断中完成 2.3 Packet Reception 接收 接收从 NIC 开始。\nNIC 接收到一对 Ethernet frames Frames 存储在 rx_ring 内核 reserved 空间中 环形缓冲区 网卡将接收到的数据放到 DMA 后，将中断信号发送给指定的 IRQ line 中断控制器负责中断指定的处理器 2.4 Incorrect SoftIRQ Accounting 3 TESTBED AND KERNEL CONFIGURATION 测试机配置和网络拓扑：\n内核参数：\nnet.core.rmem_max ， net.core.wmem_max 接收端和发送端 socket buffer size\nnet.ipv4.tcp_rmem ， net.ipv4.tcp_wmem 接收端和发送端 tcp buffer size (min, default, max)\nnet.core.netdev_max_backlog 控制软中断函数 net_rx_action() 每次处理的数据包的个数\n4 UDP TRANSMISSION 两个发送进程，MTU 9K 时候性能最好 MTU 9K 时候， packet size 8K 以上可以跑满 5 TCP TRANSMISSION shows the data transfer rate, measured as a function of the TCP send size. For a MTU of 1500 B, the maximum throughput achieved was around 5.5 Gb/s, reached at the max- imum tested send size of 64 KiB. The throughput decreased as decreased the send size, with a change in the slope at 1500 B. The adoption of the 9000 B MTU with TCP improved the throughput up to 7 Gb/s.\n6 TCP TRANSMISSION WITH ZERO-COPY sendfile() 来省略在内核态与用户态之间的数据拷贝 5.5 Gb/s -\u003e 8Gb/s 7 THE HARDWARE OFFLOAD FUNCTIONALITIES OF THE NETWORK ADAPTER offload function: 内核可以将高负载的任务交给硬件去做。\nTCP Segmentation Offload, TSO\n工作在发送端 当 TCP 的数据包大小超过 MTU 时候，必须进行分片操作 不支持 TSO 的硬件，必须由内核在 TPC 层完成 支持 TSO 的硬件，则可将最大 64K 的数据一次性交给网卡，由网卡来完成分片 Large Receive Offload， LRO\n工作在接收端 在 NIC 层将多个 TCP packets 重组成更大的数据包 Scatter-Gather (SG) I/O\n可以将不连续的内存地址通过 DMA 映射，减少内存拷贝 Checksum Offload, CO\nTCP 包的 checksum 计算 这些 offload 的效果:\n吞吐量结果：\nCPU 负载：\n8 FIBRE CHANNEL TO 10 GIGABIT ETHERNET TESTS 9 CONCLUSIONS MTU matters Offload matters… ","wordCount":"999","inLanguage":"zh-cn","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://yangyingchao.github.io/posts/performanceof10gbe/"},"publisher":{"@type":"Organization","name":"MyNotes","logo":{"@type":"ImageObject","url":"https://yangyingchao.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yangyingchao.github.io accesskey=h title="MyNotes (Alt + H)">MyNotes</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yangyingchao.github.io/ title=Home><span>Home</span></a></li><li><a href=https://yangyingchao.github.io/posts/ title=Archives><span>Archives</span></a></li><li><a href=https://yangyingchao.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://yangyingchao.github.io/categories/ title=Categories><span>Categories</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Performance of 10 G Ethernet Using Commodity Hardware</h1><div class=post-meta></div></header><div class=post-content><ul><li>1 <a href=#h:ff062e17-a590-447b-af94-db765fe35d49>INTRODUCTION</a></li><li>2 <a href=#h:79aa9e78-e621-4482-a37f-6c4e9faa51ed>NETWORK PROCESSING IN THE LINUX KERNEL</a><ul><li>2.1 <a href=#h:ae958029-e093-4749-8e53-86bbf3c625c0>Kernel Accounting</a></li><li>2.2 <a href=#h:becafb3e-b9d7-42ce-a7f6-ef239ea75c5a>Packet Transmission 发送</a></li><li>2.3 <a href=#h:89c95fde-f17c-4f48-a103-2c4f8a22c09c>Packet Reception 接收</a></li><li>2.4 <a href=#h:9010c7b0-db9f-4169-8ec4-b8905c4dd54a>Incorrect SoftIRQ Accounting</a></li></ul></li><li>3 <a href=#h:aaaa327c-4cf8-4784-a699-88f940bd6e60>TESTBED AND KERNEL CONFIGURATION</a></li><li>4 <a href=#h:8168452b-6b7f-4d83-91b1-33a6df8d63fc>UDP TRANSMISSION</a></li><li>5 <a href=#h:3d1a9962-b03b-481c-8098-aeaaad110f71>TCP TRANSMISSION</a></li><li>6 <a href=#h:b3dad9b5-5d55-4dbb-ad94-74e11ba80a91>TCP TRANSMISSION WITH ZERO-COPY</a></li><li>7 <a href=#h:566096d2-c034-451a-9844-517b5b69c0f0>THE HARDWARE OFFLOAD FUNCTIONALITIES OF THE NETWORK ADAPTER</a></li><li>8 <a href=#h:321ebff8-6056-49e5-9deb-bfc61ccb5011>FIBRE CHANNEL TO 10 GIGABIT ETHERNET TESTS</a></li><li>9 <a href=#h:77721e10-4994-4b5b-97c8-6d13ef405b68>CONCLUSIONS</a></li></ul><p>本文为摘录，原文为： ../pdf/3/performanceOf10GbE.pdf</p><h2 id=h:ff062e17-a590-447b-af94-db765fe35d49>1 INTRODUCTION<a hidden class=anchor aria-hidden=true href=#h:ff062e17-a590-447b-af94-db765fe35d49>#</a></h2><p>With the introduction of 10-GbE, network I/O re-entered the “fast network,
slow host” scenario that occurred with both the transitions to Fast Ethernet
and Gigabit Ethernet. Specifically, three major system bottlenecks may limit
the efficiency of high-performance I/O adapters:</p><ul><li><p>PCI-X 总线带宽</p><ul><li>PCI-X 频率 <code>133MHz</code> ，带宽 <code>8.5Gb/s</code></li><li>已被 PCI-Express (PCIe) 替代：<ul><li><code>8</code> 通道</li><li><code>20 Gb/s</code></li></ul></li></ul></li><li><p>CPU 利用率</p></li><li><p>内存带宽</p></li></ul><h2 id=h:79aa9e78-e621-4482-a37f-6c4e9faa51ed>2 NETWORK PROCESSING IN THE LINUX KERNEL<a hidden class=anchor aria-hidden=true href=#h:79aa9e78-e621-4482-a37f-6c4e9faa51ed>#</a></h2><h3 id=h:ae958029-e093-4749-8e53-86bbf3c625c0>2.1 Kernel Accounting<a hidden class=anchor aria-hidden=true href=#h:ae958029-e093-4749-8e53-86bbf3c625c0>#</a></h3><ul><li><p><strong>User</strong></p><ul><li>程序运行在用户态的时间</li></ul></li><li><p><strong>System</strong></p><ul><li>程序运行在内核态的时间</li></ul></li><li><p><strong>IRQ</strong></p><ul><li>CPU 处理硬件中断的时间</li></ul></li><li><p><strong>SoftIRQ</strong></p><ul><li>CPU 处理软中断的时间</li></ul></li></ul><h3 id=h:becafb3e-b9d7-42ce-a7f6-ef239ea75c5a>2.2 Packet Transmission 发送<a hidden class=anchor aria-hidden=true href=#h:becafb3e-b9d7-42ce-a7f6-ef239ea75c5a>#</a></h3><ul><li><p>发送队列</p><ul><li>每个网卡驱动维持一个数据包的发送队列</li><li>内核根据 qdisc (queue discipline) 将数据包插入到队列中<ul><li>默认的 qdisc 为 <code>pfifo_fast</code> (paccket FIO)</li><li>Linux 支持其他的策略, 如<ul><li>RED (Radom Early Drop)</li><li>CBQ (Class Based Queuing)</li><li>Others</li></ul></li></ul></li></ul></li><li><p>link layer</p><ul><li>triggered by function <code>dev_queue_xmit()</code>, 该函数负责：<ul><li>将数据包根据 <code>qdisc</code> 插入到发送队列中</li><li>从发送队列中取出待发送的包，调用驱动的发送函数 <code>hard_start_xmit()</code> 来发送</li><li>如果因为某些原因 （如设备没有资源了），它会安排 SoftIRQ ， 然后晚些通过软中断再次发送</li></ul></li></ul></li><li><p>设备驱动</p><ul><li>设备驱动负责将数据从 <code>tx_ring</code> 中转移到网卡的缓冲区中</li><li>该操作通过设置 DMA 映射，然后设置硬件上的特定寄存器来完成</li><li>驱动无需等待发送完成：<ul><li>发送完成之后硬件发出硬中断 <code>DMAdone</code></li><li>内核收到中断后安排 SoftIRQ 来释放 packet 使用的内存
释放内存操作耗时相对较长，不适合在硬中断中完成</li></ul></li></ul></li></ul><h3 id=h:89c95fde-f17c-4f48-a103-2c4f8a22c09c>2.3 Packet Reception 接收<a hidden class=anchor aria-hidden=true href=#h:89c95fde-f17c-4f48-a103-2c4f8a22c09c>#</a></h3><p>接收从 NIC 开始。</p><ul><li>NIC<ul><li>接收到一对 Ethernet frames</li><li>Frames 存储在 <code>rx_ring</code><ul><li>内核 reserved 空间中</li><li>环形缓冲区</li></ul></li><li>网卡将接收到的数据放到 DMA 后，将中断信号发送给指定的 IRQ line<ul><li>中断控制器负责中断指定的处理器</li></ul></li></ul></li></ul><h3 id=h:9010c7b0-db9f-4169-8ec4-b8905c4dd54a>2.4 Incorrect SoftIRQ Accounting<a hidden class=anchor aria-hidden=true href=#h:9010c7b0-db9f-4169-8ec4-b8905c4dd54a>#</a></h3><h2 id=h:aaaa327c-4cf8-4784-a699-88f940bd6e60>3 TESTBED AND KERNEL CONFIGURATION<a hidden class=anchor aria-hidden=true href=#h:aaaa327c-4cf8-4784-a699-88f940bd6e60>#</a></h2><ul><li><p>测试机配置和网络拓扑：</p><p></p><figure><img loading=lazy src=/ox-hugo/screenshot@2022-10-21_13:31:24.png></figure></li><li><p>内核参数：</p><p></p><figure><img loading=lazy src=/ox-hugo/screenshot@2022-10-21_13:35:13.png></figure><ul><li><p><code>net.core.rmem_max</code> ， <code>net.core.wmem_max</code>
接收端和发送端 <strong>socket</strong> buffer size</p></li><li><p><code>net.ipv4.tcp_rmem</code> ， <code>net.ipv4.tcp_wmem</code>
接收端和发送端 <strong>tcp</strong> buffer size (min, default, max)</p></li><li><p><code>net.core.netdev_max_backlog</code>
控制软中断函数 <code>net_rx_action()</code> 每次处理的数据包的个数</p></li></ul></li></ul><h2 id=h:8168452b-6b7f-4d83-91b1-33a6df8d63fc>4 UDP TRANSMISSION<a hidden class=anchor aria-hidden=true href=#h:8168452b-6b7f-4d83-91b1-33a6df8d63fc>#</a></h2><p></p><figure><img loading=lazy src=/ox-hugo/screenshot@2022-10-21_13:44:24.png></figure><ul><li>两个发送进程，MTU 9K 时候性能最好</li><li>MTU 9K 时候， packet size 8K 以上可以跑满</li></ul><h2 id=h:3d1a9962-b03b-481c-8098-aeaaad110f71>5 TCP TRANSMISSION<a hidden class=anchor aria-hidden=true href=#h:3d1a9962-b03b-481c-8098-aeaaad110f71>#</a></h2><ul><li><p>shows the data transfer rate, measured as a function of the TCP send
size. For a MTU of 1500 B, the maximum throughput achieved was around 5.5
Gb/s, reached at the max- imum tested send size of 64 KiB. The throughput
decreased as decreased the send size, with a change in the slope at 1500
B. The adoption of the 9000 B MTU with TCP improved the throughput up to 7
Gb/s.</p><p></p><figure><img loading=lazy src=/ox-hugo/screenshot@2022-10-21_13:53:04.png></figure></li></ul><h2 id=h:b3dad9b5-5d55-4dbb-ad94-74e11ba80a91>6 TCP TRANSMISSION WITH ZERO-COPY<a hidden class=anchor aria-hidden=true href=#h:b3dad9b5-5d55-4dbb-ad94-74e11ba80a91>#</a></h2><ul><li><code>sendfile()</code> 来省略在内核态与用户态之间的数据拷贝</li><li><code>5.5 Gb/s</code> -> <code>8Gb/s</code></li></ul><h2 id=h:566096d2-c034-451a-9844-517b5b69c0f0>7 THE HARDWARE OFFLOAD FUNCTIONALITIES OF THE NETWORK ADAPTER<a hidden class=anchor aria-hidden=true href=#h:566096d2-c034-451a-9844-517b5b69c0f0>#</a></h2><p>offload function: 内核可以将高负载的任务交给硬件去做。</p><ul><li><p>TCP Segmentation Offload, TSO</p><ul><li>工作在发送端</li><li>当 TCP 的数据包大小超过 MTU 时候，必须进行分片操作</li><li>不支持 TSO 的硬件，必须由内核在 TPC 层完成</li><li>支持 TSO 的硬件，则可将最大 <code>64K</code> 的数据一次性交给网卡，由网卡来完成分片</li></ul></li><li><p>Large Receive Offload， LRO</p><ul><li>工作在接收端</li><li>在 NIC 层将多个 TCP packets 重组成更大的数据包</li></ul></li><li><p>Scatter-Gather (SG) I/O</p><ul><li>可以将不连续的内存地址通过 DMA 映射，减少内存拷贝</li></ul></li><li><p>Checksum Offload, CO</p><ul><li>TCP 包的 checksum 计算</li></ul></li></ul><p>这些 offload 的效果:</p><ul><li><p>吞吐量结果：</p><p></p><figure><img loading=lazy src=/ox-hugo/screenshot@2022-10-21_17:21:40.png></figure></li><li><p>CPU 负载：</p><p></p><figure><img loading=lazy src=/ox-hugo/screenshot@2022-10-21_17:24:28.png width=800px></figure></li></ul><h2 id=h:321ebff8-6056-49e5-9deb-bfc61ccb5011>8 FIBRE CHANNEL TO 10 GIGABIT ETHERNET TESTS<a hidden class=anchor aria-hidden=true href=#h:321ebff8-6056-49e5-9deb-bfc61ccb5011>#</a></h2><h2 id=h:77721e10-4994-4b5b-97c8-6d13ef405b68>9 CONCLUSIONS<a hidden class=anchor aria-hidden=true href=#h:77721e10-4994-4b5b-97c8-6d13ef405b68>#</a></h2><ol><li>MTU matters</li><li>Offload matters…</li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://yangyingchao.github.io/tags/ethernet/>ethernet</a></li><li><a href=https://yangyingchao.github.io/tags/performance/>performance</a></li><li><a href=https://yangyingchao.github.io/tags/nic/>nic</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://yangyingchao.github.io>MyNotes</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>