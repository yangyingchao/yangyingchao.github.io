<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Performance of 10 G Ethernet Using Commodity Hardware - 杂七杂八随手记</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="yyc"><meta name=description content="Table of Contents 1 INTRODUCTION 2 NETWORK PROCESSING IN THE LINUX KERNEL 2.1 Kernel Accounting 2.2 Packet Transmission 发送 2.3 Packet Reception 接收 2.4 Incorrect SoftIRQ Accounting 3 TESTBED AND KERNEL CONFIGURATION 4 UDP TRANSMISSION 5 TCP TRANSMISSION 6 TCP TRANSMISSION WITH ZERO-COPY 7 THE HARDWARE OFFLOAD FUNCTIONALITIES OF THE NETWORK ADAPTER 8 FIBRE CHANNEL TO 10 GIGABIT ETHERNET TESTS 9 CONCLUSIONS 1 INTRODUCTION With the introduction of"><meta name=keywords content="Hugo,theme,even"><meta name=generator content="Hugo 0.115.0 with theme even"><link rel=canonical href=https://yangyingchao.github.io/posts/performanceof10gbe/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link href=/sass/main.min.154ed883776547b0e136be39b3037f61350da06f888d0868d1756a9463cd9520.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="Performance of 10 G Ethernet Using Commodity Hardware"><meta property="og:description" content="Table of Contents 1 INTRODUCTION 2 NETWORK PROCESSING IN THE LINUX KERNEL 2.1 Kernel Accounting 2.2 Packet Transmission 发送 2.3 Packet Reception 接收 2.4 Incorrect SoftIRQ Accounting 3 TESTBED AND KERNEL CONFIGURATION 4 UDP TRANSMISSION 5 TCP TRANSMISSION 6 TCP TRANSMISSION WITH ZERO-COPY 7 THE HARDWARE OFFLOAD FUNCTIONALITIES OF THE NETWORK ADAPTER 8 FIBRE CHANNEL TO 10 GIGABIT ETHERNET TESTS 9 CONCLUSIONS 1 INTRODUCTION With the introduction of"><meta property="og:type" content="article"><meta property="og:url" content="https://yangyingchao.github.io/posts/performanceof10gbe/"><meta property="article:section" content="posts"><meta itemprop=name content="Performance of 10 G Ethernet Using Commodity Hardware"><meta itemprop=description content="Table of Contents 1 INTRODUCTION 2 NETWORK PROCESSING IN THE LINUX KERNEL 2.1 Kernel Accounting 2.2 Packet Transmission 发送 2.3 Packet Reception 接收 2.4 Incorrect SoftIRQ Accounting 3 TESTBED AND KERNEL CONFIGURATION 4 UDP TRANSMISSION 5 TCP TRANSMISSION 6 TCP TRANSMISSION WITH ZERO-COPY 7 THE HARDWARE OFFLOAD FUNCTIONALITIES OF THE NETWORK ADAPTER 8 FIBRE CHANNEL TO 10 GIGABIT ETHERNET TESTS 9 CONCLUSIONS 1 INTRODUCTION With the introduction of"><meta itemprop=wordCount content="991"><meta itemprop=keywords content="ethernet,performance,nic,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Performance of 10 G Ethernet Using Commodity Hardware"><meta name=twitter:description content="Table of Contents 1 INTRODUCTION 2 NETWORK PROCESSING IN THE LINUX KERNEL 2.1 Kernel Accounting 2.2 Packet Transmission 发送 2.3 Packet Reception 接收 2.4 Incorrect SoftIRQ Accounting 3 TESTBED AND KERNEL CONFIGURATION 4 UDP TRANSMISSION 5 TCP TRANSMISSION 6 TCP TRANSMISSION WITH ZERO-COPY 7 THE HARDWARE OFFLOAD FUNCTIONALITIES OF THE NETWORK ADAPTER 8 FIBRE CHANNEL TO 10 GIGABIT ETHERNET TESTS 9 CONCLUSIONS 1 INTRODUCTION With the introduction of"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>Even</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>Home</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a><a href=/tags/><li class=mobile-menu-item>Tags</li></a><a href=/categories/><li class=mobile-menu-item>Categories</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>Even</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>Home</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=/categories/>Categories</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><div class=post-content><div class="ox-hugo-toc toc has-section-numbers"><div class=heading>Table of Contents</div><ul><li><span class=section-num>1</span> <a href=#h:ff062e17-a590-447b-af94-db765fe35d49>INTRODUCTION</a></li><li><span class=section-num>2</span> <a href=#h:79aa9e78-e621-4482-a37f-6c4e9faa51ed>NETWORK PROCESSING IN THE LINUX KERNEL</a><ul><li><span class=section-num>2.1</span> <a href=#h:ae958029-e093-4749-8e53-86bbf3c625c0>Kernel Accounting</a></li><li><span class=section-num>2.2</span> <a href=#h:becafb3e-b9d7-42ce-a7f6-ef239ea75c5a>Packet Transmission 发送</a></li><li><span class=section-num>2.3</span> <a href=#h:89c95fde-f17c-4f48-a103-2c4f8a22c09c>Packet Reception 接收</a></li><li><span class=section-num>2.4</span> <a href=#h:9010c7b0-db9f-4169-8ec4-b8905c4dd54a>Incorrect SoftIRQ Accounting</a></li></ul></li><li><span class=section-num>3</span> <a href=#h:aaaa327c-4cf8-4784-a699-88f940bd6e60>TESTBED AND KERNEL CONFIGURATION</a></li><li><span class=section-num>4</span> <a href=#h:8168452b-6b7f-4d83-91b1-33a6df8d63fc>UDP TRANSMISSION</a></li><li><span class=section-num>5</span> <a href=#h:3d1a9962-b03b-481c-8098-aeaaad110f71>TCP TRANSMISSION</a></li><li><span class=section-num>6</span> <a href=#h:b3dad9b5-5d55-4dbb-ad94-74e11ba80a91>TCP TRANSMISSION WITH ZERO-COPY</a></li><li><span class=section-num>7</span> <a href=#h:566096d2-c034-451a-9844-517b5b69c0f0>THE HARDWARE OFFLOAD FUNCTIONALITIES OF THE NETWORK ADAPTER</a></li><li><span class=section-num>8</span> <a href=#h:321ebff8-6056-49e5-9deb-bfc61ccb5011>FIBRE CHANNEL TO 10 GIGABIT ETHERNET TESTS</a></li><li><span class=section-num>9</span> <a href=#h:77721e10-4994-4b5b-97c8-6d13ef405b68>CONCLUSIONS</a></li></ul></div><h2 id=h:ff062e17-a590-447b-af94-db765fe35d49><span class=section-num>1</span> INTRODUCTION</h2><p>With the introduction of 10-GbE, network I/O re-entered the “fast network,
slow host” scenario that occurred with both the transitions to Fast Ethernet
and Gigabit Ethernet. Specifically, three major system bottlenecks may limit
the efficiency of high-performance I/O adapters:</p><ul><li><p>PCI-X 总线带宽</p><ul><li>PCI-X 频率 <code>133MHz</code> ，带宽 <code>8.5Gb/s</code></li><li>已被 PCI-Express (PCIe) 替代：<ul><li><code>8</code> 通道</li><li><code>20 Gb/s</code></li></ul></li></ul></li><li><p>CPU 利用率</p></li><li><p>内存带宽</p></li></ul><h2 id=h:79aa9e78-e621-4482-a37f-6c4e9faa51ed><span class=section-num>2</span> NETWORK PROCESSING IN THE LINUX KERNEL</h2><h3 id=h:ae958029-e093-4749-8e53-86bbf3c625c0><span class=section-num>2.1</span> Kernel Accounting</h3><ul><li><p><strong>User</strong></p><ul><li>程序运行在用户态的时间</li></ul></li><li><p><strong>System</strong></p><ul><li>程序运行在内核态的时间</li></ul></li><li><p><strong>IRQ</strong></p><ul><li>CPU 处理硬件中断的时间</li></ul></li><li><p><strong>SoftIRQ</strong></p><ul><li>CPU 处理软中断的时间</li></ul></li></ul><h3 id=h:becafb3e-b9d7-42ce-a7f6-ef239ea75c5a><span class=section-num>2.2</span> Packet Transmission 发送</h3><ul><li><p>发送队列</p><ul><li>每个网卡驱动维持一个数据包的发送队列</li><li>内核根据 qdisc (queue discipline) 将数据包插入到队列中<ul><li>默认的 qdisc 为 <code>pfifo_fast</code> (paccket FIO)</li><li>Linux 支持其他的策略, 如<ul><li>RED (Radom Early Drop)</li><li>CBQ (Class Based Queuing)</li><li>Others</li></ul></li></ul></li></ul></li><li><p>link layer</p><ul><li>triggered by function <code>dev_queue_xmit()</code>, 该函数负责：<ul><li>将数据包根据 <code>qdisc</code> 插入到发送队列中</li><li>从发送队列中取出待发送的包，调用驱动的发送函数 <code>hard_start_xmit()</code> 来发送</li><li>如果因为某些原因 （如设备没有资源了），它会安排 SoftIRQ ， 然后晚些通过软中断再次发送</li></ul></li></ul></li><li><p>设备驱动</p><ul><li>设备驱动负责将数据从 <code>tx_ring</code> 中转移到网卡的缓冲区中</li><li>该操作通过设置 DMA 映射，然后设置硬件上的特定寄存器来完成</li><li>驱动无需等待发送完成：<ul><li>发送完成之后硬件发出硬中断 <code>DMAdone</code></li><li>内核收到中断后安排 SoftIRQ 来释放 packet 使用的内存<br>释放内存操作耗时相对较长，不适合在硬中断中完成</li></ul></li></ul></li></ul><h3 id=h:89c95fde-f17c-4f48-a103-2c4f8a22c09c><span class=section-num>2.3</span> Packet Reception 接收</h3><p>接收从 NIC 开始。</p><ul><li>NIC<ul><li>接收到一对 Ethernet frames</li><li>Frames 存储在 <code>rx_ring</code><ul><li>内核 reserved 空间中</li><li>环形缓冲区</li></ul></li><li>网卡将接收到的数据放到 DMA 后，将中断信号发送给指定的 IRQ line<ul><li>中断控制器负责中断指定的处理器</li></ul></li></ul></li></ul><h3 id=h:9010c7b0-db9f-4169-8ec4-b8905c4dd54a><span class=section-num>2.4</span> Incorrect SoftIRQ Accounting</h3><h2 id=h:aaaa327c-4cf8-4784-a699-88f940bd6e60><span class=section-num>3</span> TESTBED AND KERNEL CONFIGURATION</h2><ul><li><p>测试机配置和网络拓扑：</p><p><a id=figure--fig:screenshot@2022-10-21-13:31:24></a></p><figure><img src=/ox-hugo/screenshot@2022-10-21_13:31:24.png></figure></li><li><p>内核参数：</p><p><a id=figure--fig:screenshot@2022-10-21-13:35:13></a></p><figure><img src=/ox-hugo/screenshot@2022-10-21_13:35:13.png></figure><ul><li><p><code>net.core.rmem_max</code> ， <code>net.core.wmem_max</code><br>接收端和发送端 <strong>socket</strong> buffer size</p></li><li><p><code>net.ipv4.tcp_rmem</code> ， <code>net.ipv4.tcp_wmem</code><br>接收端和发送端 <strong>tcp</strong> buffer size (min, default, max)</p></li><li><p><code>net.core.netdev_max_backlog</code><br>控制软中断函数 <code>net_rx_action()</code> 每次处理的数据包的个数</p></li></ul></li></ul><h2 id=h:8168452b-6b7f-4d83-91b1-33a6df8d63fc><span class=section-num>4</span> UDP TRANSMISSION</h2><p><a id=figure--fig:screenshot@2022-10-21-13:44:24></a></p><figure><img src=/ox-hugo/screenshot@2022-10-21_13:44:24.png></figure><ul><li>两个发送进程，MTU 9K 时候性能最好</li><li>MTU 9K 时候， packet size 8K 以上可以跑满</li></ul><h2 id=h:3d1a9962-b03b-481c-8098-aeaaad110f71><span class=section-num>5</span> TCP TRANSMISSION</h2><ul><li><p>shows the data transfer rate, measured as a function of the TCP send
size. For a MTU of 1500 B, the maximum throughput achieved was around 5.5
Gb/s, reached at the max- imum tested send size of 64 KiB. The throughput
decreased as decreased the send size, with a change in the slope at 1500
B. The adoption of the 9000 B MTU with TCP improved the throughput up to 7
Gb/s.</p><p><a id=figure--fig:screenshot@2022-10-21-13:53:04></a></p><figure><img src=/ox-hugo/screenshot@2022-10-21_13:53:04.png></figure></li></ul><h2 id=h:b3dad9b5-5d55-4dbb-ad94-74e11ba80a91><span class=section-num>6</span> TCP TRANSMISSION WITH ZERO-COPY</h2><ul><li><code>sendfile()</code> 来省略在内核态与用户态之间的数据拷贝</li><li><code>5.5 Gb/s</code> -> <code>8Gb/s</code></li></ul><h2 id=h:566096d2-c034-451a-9844-517b5b69c0f0><span class=section-num>7</span> THE HARDWARE OFFLOAD FUNCTIONALITIES OF THE NETWORK ADAPTER</h2><p>offload function: 内核可以将高负载的任务交给硬件去做。</p><ul><li><p>TCP Segmentation Offload, TSO</p><ul><li>工作在发送端</li><li>当 TCP 的数据包大小超过 MTU 时候，必须进行分片操作</li><li>不支持 TSO 的硬件，必须由内核在 TPC 层完成</li><li>支持 TSO 的硬件，则可将最大 <code>64K</code> 的数据一次性交给网卡，由网卡来完成分片</li></ul></li><li><p>Large Receive Offload， LRO</p><ul><li>工作在接收端</li><li>在 NIC 层将多个 TCP packets 重组成更大的数据包</li></ul></li><li><p>Scatter-Gather (SG) I/O</p><ul><li>可以将不连续的内存地址通过 DMA 映射，减少内存拷贝</li></ul></li><li><p>Checksum Offload, CO</p><ul><li>TCP 包的 checksum 计算</li></ul></li></ul><p>这些 offload 的效果:</p><ul><li><p>吞吐量结果：</p><p><a id=figure--fig:screenshot@2022-10-21-17:21:40></a></p><figure><img src=/ox-hugo/screenshot@2022-10-21_17:21:40.png></figure></li><li><p>CPU 负载：</p><p><a id=figure--fig:screenshot@2022-10-21-17:24:28></a></p><figure><img src=/ox-hugo/screenshot@2022-10-21_17:24:28.png width=800px></figure></li></ul><h2 id=h:321ebff8-6056-49e5-9deb-bfc61ccb5011><span class=section-num>8</span> FIBRE CHANNEL TO 10 GIGABIT ETHERNET TESTS</h2><h2 id=h:77721e10-4994-4b5b-97c8-6d13ef405b68><span class=section-num>9</span> CONCLUSIONS</h2><ol><li>MTU matters</li><li>Offload matters…</li></ol></div></article></div></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:yang.yingchao@qq.com class="iconfont icon-email" title=email></a>
<a href=http://localhost:1313 class="iconfont icon-stack-overflow" title=stack-overflow></a>
<a href=https://github.com/yangyingchao/ class="iconfont icon-github" title=github></a>
<a href=https://yangyingchao.github.io/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=power-by>由 <a class=hexo-link href=https://gohugo.io>Hugo</a> 强力驱动</span>
<span class=division>|</span>
<span class=theme-info>主题 -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a></span>
<span class=copyright-year>&copy;
2017 -
2023<span class=heart><i class="iconfont icon-heart"></i></span><span>yyc</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js></script></body></html>